{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Lab: 9.6.1 Support Vector Classifier](#9.6.1-Support-Vector-Classifier)\n",
    "- [Lab: 9.6.2 Support Vector Machine](#9.6.2-Support-Vector-Machine)\n",
    "- [Lab: 9.6.3 ROC Curves](#9.6.3-ROC-Curves)\n",
    "- [Lab: 9.6.4 SVM with Multiple Classes](#9.6.4-SVM-with-Multiple-Classes)\n",
    "- [Lab: 9.6.5 Application to Gene Expression Data](#9.6.5-Application-to-Gene-Expression-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    " \n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_df(clf, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    try:\n",
    "        cm_df = pd.DataFrame(cm.T, index=clf.classes_, columns=clf.classes_)\n",
    "    except:\n",
    "        cm_df = pd.DataFrame(cm.T)\n",
    "    cm_df.index.name = 'Predicted'\n",
    "    cm_df.columns.name = 'True'\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM\n",
    "\n",
    "Hyperparameter __`C`__ is the cost of misclassification:\n",
    " - reducing C means less misclassification cost, expect more misclassifications\n",
    " - increases the boundary margin\n",
    " - increases bias (misclassifications)\n",
    " - lowers variance and as result overfitting\n",
    " - the default value for parameter `C` is 1.0\n",
    " \n",
    "For RBF kernel - hyperparameter __Sigma__ (std. deviation):\n",
    "\n",
    "- sigma plays an role to be an amplifier of the distance between x and x'\n",
    "- when the distance between x and x' is much larger than sigma, the kernel function tends to be zero. \n",
    "- if the sigma is very small, only the x within the certain distance can affect the predicting point. \n",
    "\n",
    "As for the variance and bias explanation, \n",
    " - smaller sigma => less bias and more variance \n",
    " - larger sigma => less variance and more bias => more smooth boundary and less overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.1 Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot a classifier with support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
    "    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Support vectors indicated in plot by vertical lines\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(sv[:,0], sv[:,1], c='k', marker='+', s=100, linewidths='1')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.show()\n",
    "    print('Number of support vectors: ', svc.support_.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data: 20 observations of 2 features and divide into two classes.\n",
    "np.random.seed(5)\n",
    "X = np.random.randn(20, 2)\n",
    "y = np.repeat([1, -1], 10)\n",
    "\n",
    "X[y == -1] = X[y == -1] +1\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C= 1.0, kernel='linear')\n",
    "svc.fit(X, y)\n",
    "\n",
    "plot_svc(svc, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a smaller cost parameter (C=0.1) the margin is wider, resulting in more support vectors.\n",
    "svc2 = SVC(C=0.1, kernel='linear')\n",
    "svc2.fit(X, y)\n",
    "plot_svc(svc2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the optimal C parameter by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'C': [0.001, 0.01, 0.1, 1, 5, 10, 100]}]\n",
    "clf = GridSearchCV(SVC(kernel='linear'), \n",
    "                   tuned_parameters, \n",
    "                   cv=10, \n",
    "                   scoring='accuracy')\n",
    "clf.fit(X, y)\n",
    "# clf.grid_scores_  # deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(param, score) for param, score in zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score'])], key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best hyperparameter (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_test = np.random.randn(20, 2)\n",
    "y_test = np.random.choice([-1, 1], 20)\n",
    "X_test[y_test==1] = X_test[y_test==1] -1  # shift X to make more separable\n",
    "\n",
    "plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=plt.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc2 : C = 0.1\n",
    "y_pred = svc2.predict(X_test)\n",
    "confusion_df(svc2, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc3 = SVC(C=0.001, kernel='linear').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc3 : C = 0.001\n",
    "y_pred = svc3.predict(X_test)\n",
    "confusion_df(svc3, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the test data so that the classes are really seperable with a hyperplane.\n",
    "X_test[y_test==1] = X_test[y_test==1] -1\n",
    "\n",
    "plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=plt.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc4 = SVC(C=10.0, kernel='linear').fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svc4, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: 10.0 => 1.0, (less misclassification cost) increases the margin:\n",
    "#  Now there is one misclassification: increased bias, lower variance.\n",
    "svc5 = SVC(C=1.0, kernel='linear').fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svc5, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.2 Support Vector Machine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "X = np.random.randn(200,2)\n",
    "X[:100] = X[:100] +2\n",
    "X[101:150] = X[101:150] -2\n",
    "y = np.concatenate([np.repeat(-1, 150), np.repeat(1,50)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1.0, kernel='rbf', gamma=1.0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing C parameter, allowing more flexibility\n",
    "svm2 = SVC(C=100, kernel='rbf', gamma=1.0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svm2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'C': [0.01, 0.1, 1, 10, 100],\n",
    "                     'gamma': [0.5, 1, 2, 3, 4]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf'), tuned_parameters, cv=10, scoring='accuracy', return_train_score=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorted list of CV scores and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(param, score) for param, score in zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score'])], key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameters - estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, clf.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.3 ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the ROC curves of two models on train/test data. One model is more flexible than the other.  \n",
    "NOTE: multiclass format is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm3 = SVC(C=1, kernel='rbf', gamma=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More flexible model\n",
    "svm4 = SVC(C=1, kernel='rbf', gamma=50).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision function - train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score3 = svm3.decision_function(X_train)\n",
    "y_train_score4 = svm4.decision_function(X_train)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision function - test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_score3_t = svm3.decision_function(X_test)\n",
    "y_test_score4_t = svm4.decision_function(X_test)\n",
    "\n",
    "false_pos_rate3_t, true_pos_rate3_t, _ = roc_curve(y_test, y_test_score3_t)\n",
    "roc_auc3_t = auc(false_pos_rate3_t, true_pos_rate3_t)\n",
    "\n",
    "false_pos_rate4_t, true_pos_rate4_t, _ = roc_curve(y_test, y_test_score4_t)\n",
    "roc_auc4_t = auc(false_pos_rate4_t, true_pos_rate4_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = lambda g, auc: 'SVM $\\gamma = {}$ ROC curve (area = {:.2f})'.format(g, auc)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "_ = ax1.plot(false_pos_rate3, true_pos_rate3, label=label(2, roc_auc3), color='b')\n",
    "_ = ax1.plot(false_pos_rate4, true_pos_rate4, label=label(50, roc_auc4), color='r')\n",
    "_ = ax1.set_title('Training Data')\n",
    "\n",
    "_ = ax2.plot(false_pos_rate3_t, true_pos_rate3_t, label=label(2, roc_auc3_t), color='b')\n",
    "_ = ax2.plot(false_pos_rate4_t, true_pos_rate4_t, label=label(50, roc_auc4_t), color='r')\n",
    "_ = ax2.set_title('Test Data')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    _ = ax.plot([0, 1], [0, 1], 'k--')\n",
    "    _ = ax.set_xlim([-0.05, 1.0])\n",
    "    _ = ax.set_ylim([0.0, 1.05])\n",
    "    _ = ax.set_xlabel('False Positive Rate')\n",
    "    _ = ax.set_ylabel('True Positive Rate')\n",
    "    _ = ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the more flexible model scores better on training data but worse on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.4 SVM with Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mulitclass synthetic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third class of observations\n",
    "np.random.seed(8)\n",
    "XX = np.vstack([X, np.random.randn(50, 2)])\n",
    "yy = np.hstack([y, np.repeat(0, 50)])\n",
    "XX[yy ==0] = XX[yy == 0] +4\n",
    "\n",
    "plt.scatter(XX[:,0], XX[:,1], s=70, c=yy, cmap=plt.cm.prism)\n",
    "plt.xlabel('XX1')\n",
    "plt.ylabel('XX2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm5 = SVC(C=1, kernel='rbf').fit(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svm5, XX, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.5 Application to Gene Expression Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../../_data | grep -i khan_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../../_data/Khan_xtrain.csv').drop('Unnamed: 0', axis=1)\n",
    "y_train = pd.read_csv('../../_data/Khan_ytrain.csv').drop('Unnamed: 0', axis=1).as_matrix().ravel()\n",
    "X_test = pd.read_csv('../../_data/Khan_xtest.csv').drop('Unnamed: 0', axis=1)\n",
    "y_test = pd.read_csv('../../_data/Khan_ytest.csv').drop('Unnamed: 0', axis=1).as_matrix().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train counts\n",
    "pd.Series(y_train).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test counts\n",
    "pd.Series(y_test).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model gives identical results to the svm() of the R package e1071, also based on libsvm library.\n",
    "svc = SVC(kernel='linear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_df(svc, y_train, svc.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_df(svc, y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
