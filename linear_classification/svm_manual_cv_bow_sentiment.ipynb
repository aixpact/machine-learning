{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with support vector machines\n",
    "\n",
    "In this notebook, we will revisit a learning task that we encountered earlier in the course: predicting the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. Previously we found a logistic regression classifier. Today we will use a support vector machine.\n",
    "\n",
    "Before starting on this notebook, make sure the folder `sentiment_labelled_sentences` (containing the data file `full_set.txt`) is in the same directory. Recall that the data can be downloaded from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing the data\n",
    " \n",
    "Here we follow exactly the same steps as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../../_data | grep -i full_set.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data set.\n",
    "with open(\"../../_data/sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted chars, symbols, digits, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    \"\"\"returns x with all the characters in removal_list replaced by a space\"\"\"\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]\n",
    "\n",
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sents_processed), sents_processed[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform sentences to Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "## Append '1' to the end of each vector.\n",
    "data_mat = data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a support vector machine to the data\n",
    "\n",
    "In support vector machines, we are given a set of examples $(x_1, y_1), \\ldots, (x_n, y_n)$ and we want to find a weight vector $w \\in \\mathbb{R}^d$ that solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{w \\in \\mathbb{R}^d} \\| w \\|^2 + C \\sum_{i=1}^n \\xi_i $$\n",
    "$$ \\text{subject to } y_i \\langle w, x_i \\rangle \\geq 1 - \\xi_i \\text{ for all } i=1,\\ldots, n$$\n",
    "\n",
    "`scikit-learn` provides an SVM solver that we will use. The following routine takes as input the constant `C` (from the above optimization problem) and returns the training and test error of the resulting SVM model. It is invoked as follows:\n",
    "\n",
    "* `training_error, test_error = fit_classifier(C)`\n",
    "\n",
    "Hyperparameter __`C`__ is the cost of misclassification:\n",
    " - reducing C means less misclassification cost, expect more misclassifications\n",
    " - increases the boundary margin\n",
    " - increases bias (misclassifications)\n",
    " - lowers variance and as result overfitting\n",
    " - the default value for parameter `C` is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classifier(C_value=1.0):\n",
    "    clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
    "    clf.fit(train_data,train_labels)\n",
    "    \n",
    "    ## Get predictions on training data\n",
    "    train_preds = clf.predict(train_data)\n",
    "    train_error = float(np.sum((train_preds > 0.0) != (train_labels > 0.0)))/len(train_labels)\n",
    "    \n",
    "    ## Get predictions on test data\n",
    "    test_preds = clf.predict(test_data)\n",
    "    test_error = float(np.sum((test_preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvals = [0.01,0.1,1.0,10.0,100.0,1000.0]\n",
    "for c in cvals:\n",
    "    train_error, test_error = fit_classifier(c)\n",
    "    print(\"C: {:.3f} \\ttrain-error: {:0.3f} test-error: {:0.3f}\".format(c, train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating C by k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_error(x, y, C_value, k):\n",
    "    \n",
    "    n = len(y)\n",
    "    fold = int(n/k)\n",
    "    ## Randomly shuffle indices\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    ## Initialize error\n",
    "    err = 0.0\n",
    "    \n",
    "    ## Iterate over partitions\n",
    "    for i in range(k):\n",
    "        \n",
    "        ## Split train-test indices\n",
    "        test_indices = indices[i*fold:(i+1)*fold]\n",
    "        train_indices = np.setdiff1d(indices, test_indices)\n",
    "        \n",
    "        ## Train classifier with parameter c\n",
    "        clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
    "        clf.fit(x[train_indices], y[train_indices])\n",
    "        \n",
    "        ## Get predictions on test partition\n",
    "        preds = clf.predict(x[test_indices])\n",
    "        \n",
    "        ## Compute error\n",
    "        err += float(np.sum((preds > 0.0) != (y[test_indices] > 0.0)))/len(test_indices)\n",
    "        \n",
    "    return err/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter (C) optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure **cross_validation_error** (above) evaluates a single candidate value of `C`. We need to use it repeatedly to identify a good `C`. \n",
    "\n",
    "* `c, err = choose_parameter(x,y,k)`\n",
    "\n",
    "where\n",
    "* `x,y` is the training data\n",
    "* `k` is the number of folds of cross-validation\n",
    "* `c` is chosen value of the parameter `C`\n",
    "* `err` is the cross-validation error estimate at `c`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively narrow down the best C-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "def zoom_range(x, y, k, c, err, low, hi):\n",
    "    \n",
    "    # Base case - plot results\n",
    "    if hi - low < 0.05:\n",
    "#         print('found in: [{:.3f} < {:.3f} < {:.3f}]'.format(low, err, hi))\n",
    "        fig, ax = plt.figure(), plt.gca()\n",
    "        ax.scatter([x for x, y in plot_data], [y for x, y in plot_data], linewidth=2, color='green')\n",
    "        ax.set_xscale('log')\n",
    "        plt.xlabel('C')\n",
    "        plt.ylabel('Error')\n",
    "        plt.show()\n",
    "        return (c, err)\n",
    "    \n",
    "    # Reset hyperparam range\n",
    "    c_space = np.linspace(low, hi, 5)\n",
    "    err_space = np.zeros(5)\n",
    "    \n",
    "    # Get CV error scores\n",
    "    for i, c in enumerate(c_space):\n",
    "        err_space[i] = cross_validation_error(x, y, c, k)\n",
    "        plot_data.append([c, err_space[i]])\n",
    "#         print('index: {}, error: {:.3f}, C: {:.3f} [{:.3f} - {:.3f}]'.format(i, err_space[i], c, low,  hi))\n",
    "\n",
    "    # Recursive call - on narrowed down hyperparam space\n",
    "    \n",
    "    # if current min hyperparam value is min => min/=4, max=min*2\n",
    "    if np.argmin(err_space) == 0:\n",
    "        return zoom_range(x, y, k, c_space[0], err_space[0], c_space[0]/4, c_space[0]*2)\n",
    "    \n",
    "    # if current max hyperparam value is min => min/=2, max=min*4\n",
    "    elif np.argmin(err_space) == 4:\n",
    "        return zoom_range(x, y, k, c_space[4], err_space[4], c_space[4]/2, c_space[4]*4)\n",
    "    else:\n",
    "        # else zoom-in; remove indices 0 and 4\n",
    "        return zoom_range(x, y, k, c_space[np.argmin(err_space)], err_space[np.argmin(err_space)], \n",
    "               c_space[np.argmin(err_space)-1], c_space[np.argmin(err_space)+1])\n",
    "\n",
    "def choose_parameter(x, y, k):\n",
    "    return zoom_range(x, y, k, c=0, err=1, low=.1, hi=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best C and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, err = choose_parameter(train_data, train_labels, 10)\n",
    "\n",
    "print(\"Choice of C: \", c)\n",
    "print(\"Cross-validation error estimate: \", err)\n",
    "\n",
    "## Train it and test it\n",
    "clf = svm.LinearSVC(C=c, loss='hinge')\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_data)\n",
    "error = float(np.sum((preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
    "print(\"Test error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(index):\n",
    "    idx = test_inds[index]\n",
    "    sents_processed[idx]\n",
    "    data_mat[idx]\n",
    "    if y[idx] != clf.predict([data_mat[idx]])[0]:\n",
    "        return 'Review *****: {}. (label:{}, prediction:{})'.format(sents_processed[idx], y[idx], clf.predict([data_mat[idx]])[0])\n",
    "    return 'Review clear: {}. (sentiment:{})'.format(sents_processed[idx], clf.predict([data_mat[idx]])[0])\n",
    "\n",
    "for i in range(50):\n",
    "    sentiment(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
