{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Perceptron and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll try out the multiclass Perceptron and SVM on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass Perceptron algorithm is similar in spirit to our earlier binary Perceptron algorithm, except that now there is a linear function for each class.\n",
    "\n",
    "If there are __`k`__ classes, (`0,1,...,k-1`). For __`d`-dimensional data__, the classifier will be parametrized by:\n",
    "* __`w`__: this is a __`(kxd)` numpy array__ with one row for each class\n",
    "* __`b`__: this is a __`k`-dimensional numpy array__ with one offset for each class\n",
    "\n",
    "Thus the linear function for class `j` (where `j` lies in the range `0` to `k-1`) is given by `w[j,:], b[j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(w, b, x):\n",
    "    \"\"\"Prediction of classifier at x\"\"\"\n",
    "    k = len(b)\n",
    "    scores = np.zeros(k)\n",
    "    for j in range(k):\n",
    "        scores[j] = w[j, :] @ x + b[j]\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multiclass Perceptron\n",
    "\n",
    "where\n",
    "* `x`: n-by-d numpy array with n data points, each d-dimensional\n",
    "* `y`: n-dimensional numpy array with the labels (in the range `0` to `k-1`)\n",
    "* `k`: the number of classes\n",
    "* `n_iters`: the training procedure will run through the data at most this many times (default: 100)\n",
    "* `w,b`: parameters for the final linear classifier, as above\n",
    "* `converged`: flag (True/False) indicating whether the algorithm converged within the prescribed number of iterations\n",
    "\n",
    "__NOTE:__ If the data is not linearly separable, then the training procedure will not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_perceptron(x, y, k, n_iters=1000):\n",
    "    \n",
    "    n, d = x.shape\n",
    "    w, b = np.zeros((k, d)), np.zeros(k)\n",
    "    converged = 0\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    for itr in range(n_iters):\n",
    "        for j in np.random.permutation(n):\n",
    "            true_y, pred_y = int(y[j]), evaluate_classifier(w, b, x[j,:])\n",
    "            if pred_y != true_y:\n",
    "                w[true_y,:] += x[j,:]\n",
    "                b[true_y] += 1.0\n",
    "                w[pred_y,:] -= x[j,:]\n",
    "                b[pred_y] -= 1.0\n",
    "                converged = itr\n",
    "            \n",
    "    if converged < n_iters:\n",
    "        print(\"Perceptron algorithm: iterations until convergence: \", converged)\n",
    "    else:\n",
    "        print(\"Perceptron algorithm: did not converge within the specified number of iterations\")\n",
    "    return w, b, converged < n_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Multiclass Perceptron boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "* `x` and `y` are the two-dimensional data and their labels (in the range `0,...,k-1`)\n",
    "* `pred_fn` is the classifier: it is a function that takes a data point and returns a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data_and_boundary(x, y, pred_fn, title='Title'):\n",
    "    # Determine the x1- and x2- limits of the plot\n",
    "    x1min = min(x[:,0]) - 1\n",
    "    x1max = max(x[:,0]) + 1\n",
    "    x2min = min(x[:,1]) - 1\n",
    "    x2max = max(x[:,1]) + 1\n",
    "    plt.xlim(x1min, x1max)\n",
    "    plt.ylim(x2min, x2max)\n",
    "    \n",
    "    # Plot the data points\n",
    "    labels = np.unique(y).astype('i')\n",
    "    cols = ['ro', 'k^', 'b*','gx']\n",
    "    for label in labels:\n",
    "        plt.plot(x[(y==label), 0], x[(y==label), 1], cols[label%4], markersize=8)\n",
    "        \n",
    "    # Construct a grid of points at which to evaluate the classifier\n",
    "    density = 0.05\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1min, x1max+density, density), np.arange(x2min, x2max+density, density))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    \n",
    "    # Use prediction function\n",
    "    Z = np.array([pred_fn(pt) for pt in grid])\n",
    "    \n",
    "    # Show the classifier's boundary using a color plot\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "#     plt.pcolormesh(xx1, xx2, Z, cmap=plt.cm.Pastel1, vmin=0, vmax=k)\n",
    "    plt.contourf(xx1, xx2, Z, cmap=plt.cm.Pastel1, vmin=0, vmax=len(labels))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following procedure, **run_multiclass_perceptron**, loads a labeled two-dimensional data set, learns a linear classifier using the Perceptron algorithm, and then displays the data as well as the boundary.\n",
    "\n",
    "The data file is assumed to contain one data point per line, along with a label, like:\n",
    "* `3 8 2` (meaning that point `x=(3,8)` has label `y=2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !find ../../_data | grep -i data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(datafile):\n",
    "    \"\"\"\"\"\"\n",
    "    data = np.loadtxt(datafile)\n",
    "    x, y = data[:, 0:2], data[:, 2]\n",
    "    k = len(np.unique(y))\n",
    "    return x, y, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiclass_perceptron(datafile):\n",
    "    \"\"\"\"\"\"\n",
    "    x, y, k = get_data(datafile)\n",
    "    \n",
    "    # Run the Perceptron algorithm for at most 1000 iterations\n",
    "    w, b, converged = train_multiclass_perceptron(x, y, k, 1000)\n",
    "    \n",
    "    # Show the data and boundary\n",
    "    pred_fn = lambda p: evaluate_classifier(w, b, p)\n",
    "    display_data_and_boundary(x, y, pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_perceptron('../../_data/data_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_perceptron('../../_data/data_4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments with multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how multiclass SVM fares on these same data sets. We start with an analog of the **run_multiclass_perceptron** function. The key difference is that the SVM version, **run_multiclass_svm**, takes a second parameter: the regularization constant `C` in the convex program of the soft-margin SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiclass_svm(datafile, C_value=1.0):\n",
    "    \n",
    "    x, y, k = get_data(datafile)\n",
    "    clf = LinearSVC(loss='hinge', multi_class='crammer_singer', C=C_value).fit(x, y)\n",
    "    pred_fn = lambda p: clf.predict(p.reshape(1,-1))   \n",
    "    # Show the data and boundary\n",
    "    display_data_and_boundary(x, y, pred_fn, 'SVM, C: {}'.format(C_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this on the two data sets `data_3.txt` and `data_4.txt` that we saw earlier. Try playing with the second parameter to see how the decision boundary changes. You should try values like `C = 0.01, 0.1, 1.0, 10.0, 100.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [0.01, 0.1, 1.0, 10.0, 100.0]:\n",
    "    run_multiclass_svm('../../_data/data_3.txt', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [0.01, 0.1, 1.0, 10.0, 100.0]:\n",
    "    run_multiclass_svm('../../_data/data_4.txt', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">For you to think about:</font> How would you summarize the effect of varying `C`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRIS data set\n",
    "\n",
    "This is four-dimensional data with three labels.  \n",
    "We will pick two features, as a consequence the problem is not linearly separable.  \n",
    " - the Perceptron algorithm would never converge\n",
    " - the soft-margin SVM obtains a reasonable solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "# Select two of the four features\n",
    "x = x[:, [1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(loss='hinge', multi_class='crammer_singer').fit(x,y)\n",
    "pred_fn = lambda p: clf.predict(p.reshape(1, -1))\n",
    "\n",
    "display_data_and_boundary(x, y, pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
