{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../_data/fraud_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label stats / distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(df['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix of dummy classifier\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DummyClassifier\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "# Get scores (manually)\n",
    "cm = confusion_matrix(y_test, y_dummy_predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP = cm[0, 0], cm[0, 1]\n",
    "FN, TP = cm[1, 0], cm[1, 1]\n",
    "# or\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculated\n",
    "accuracy_sc_m = float((TN + TP) /(TN + FP + FN + TP))\n",
    "recall_sc_m = float(TP /(TP + FN))\n",
    "precision_sc_m = float(TP /(TP + FP + 10e-8))  # *avoid div/zero\n",
    "\n",
    "# Function\n",
    "accuracy_sc = accuracy_score(y_test, y_dummy_predictions)\n",
    "recall_sc = recall_score(y_test, y_dummy_predictions)\n",
    "precision_sc = precision_score(y_test, y_dummy_predictions)\n",
    "\n",
    "accuracy_sc_m, recall_sc_m, precision_sc_m\n",
    "accuracy_sc, recall_sc, precision_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC().fit(X_train, y_train)\n",
    "y_predicted = svm.predict(X_test)\n",
    "\n",
    "accuracy_sc = accuracy_score(y_test, y_predicted)\n",
    "recall_sc = recall_score(y_test, y_predicted)\n",
    "precision_sc = precision_score(y_test, y_predicted)\n",
    "\n",
    "accuracy_sc, recall_sc, precision_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix using decision function with threshold\n",
    "\n",
    " - What is the confusion matrix when using a fraud threshold of -220?\n",
    " - Decision function returns prediction values for y, which can be converted to labels using a threshold\n",
    " - One can reduce the sensitivity(recall) and reduce the number of FN/falsely convicted/negative diagnosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)\n",
    "\n",
    "fraud_threshold = np.linspace(100, 350, 6) * -1\n",
    "\n",
    "for thres in fraud_threshold:\n",
    "    y_decision_scores = svm.decision_function(X_test) > thres\n",
    "    print('Fraud threshold: {}\\n'.format(thres), confusion_matrix(y_test, y_decision_scores.astype('int')))\n",
    "    print('Sensitivity: {:.3f}\\n'.format(recall_score(y_test, y_decision_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit lr, return probabilities\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_proba = lr.predict_proba(X_test)[: ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0.0, 1.01]) # OOP\n",
    "plt.ylim([0.0, 1.01])    # pyplot\n",
    "\n",
    "\n",
    "# PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "ax.plot(precision, recall, label='Precision-Recall Curve')\n",
    "ax.set_ylabel('Recall', fontsize=16)\n",
    "ax.set_xlabel('Precision', fontsize=16)\n",
    "\n",
    "# ROC curve\n",
    "false_positive_rate, recall, _ = roc_curve(y_test, y_proba)\n",
    "ax.plot(false_positive_rate, recall, label='ROC Curve')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation by GridSearchCV\n",
    "\n",
    "- Perform a grid search over a selection of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "C = [0.01, 0.1, 1, 10, 100]\n",
    "penalty = ['l1', 'l2']\n",
    "    \n",
    "grid_values = {'C':C, 'penalty': penalty}\n",
    "grid_lr_prec = GridSearchCV(lr, \n",
    "                            param_grid=grid_values,\n",
    "                            scoring='recall',\n",
    "                            cv=5,\n",
    "                            return_train_score=True)\n",
    "\n",
    "grid_lr_prec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean test scores of each hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid_lr_prec.cv_results_)\n",
    "pivot = pd.pivot_table(df, values='mean_test_score', index=['param_C'], columns = ['param_penalty']).as_matrix()\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot mean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pivot.reshape(5, 2), xticklabels=penalty, yticklabels=C, vmin=0.77, vmax=0.81)\n",
    "plt.yticks(rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
