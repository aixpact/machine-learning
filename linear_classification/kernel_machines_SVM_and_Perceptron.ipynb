{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use simple two-dimensional data sets to illustrate the behavior of the support vector machine and the Perceptron, when used with quadratic and RBF kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    " - two-dimensional data files `data1.txt` ~ `data5.txt` contain one data coordinate/point per line, along with a label (either -1 or 1)   \n",
    " e.g.: * `3 8 -1` (meaning that point `x=(3,8)` has label `y=-1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = ['dummy0', '../../_data/data1.txt', '../../_data/data2.txt', '../../_data/data3.txt', \n",
    " '../../_data/data4.txt', '../../_data/data5.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM\n",
    "\n",
    " 1. loads one of these data sets\n",
    " 2. learns `sklearn.SVC` classifier\n",
    " 3. plot the data, support vectors and boundary\n",
    "\n",
    "Parameters:\n",
    "* `datafile` is one of `'data1.txt'` ~ `'data5.txt'` (or another file in the same format)\n",
    "* `kernel_type` is either `'quadratic'` or `'rbf'`\n",
    "* `C_value` is the setting of the soft-margin parameter `C` (default: 1.0)\n",
    "* `s_value` (for the RBF kernel) is the scaling parameter `s` (default: 1.0)\n",
    "\n",
    "Hyperparameter __`C`__ is the cost of misclassification:\n",
    " - reducing C means less misclassification cost, expect more misclassifications\n",
    " - increases the boundary margin\n",
    " - increases bias (misclassifications)\n",
    " - lowers variance and as result overfitting\n",
    " - the default value for parameter `C` is 1.0\n",
    "\n",
    "For RBF kernel - hyperparameter __Sigma__ (std. deviation):\n",
    "\n",
    "- sigma plays an role to be an amplifier of the distance between x and x'\n",
    "- when the distance between x and x' is much larger than sigma, the kernel function tends to be zero. \n",
    "- if the sigma is very small, only the x within the certain distance can affect the predicting point. \n",
    "\n",
    "As for the variance and bias explanation, \n",
    " - smaller sigma => less bias and more variance \n",
    " - larger sigma => less variance and more bias => more smooth boundary and less overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper for grid pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_grid(x, y, ax_pad=0, density=0.1):\n",
    "    \"\"\"returns grid of (xx, yy) pairs, w.r.t. density and padding\"\"\"\n",
    "    xmin, xmax = min(x)-ax_pad, max(x)+ax_pad\n",
    "    ymin, ymax = min(y)-ax_pad, max(y)+ax_pad\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax+density, density), np.arange(ymin, ymax+density, density))\n",
    "    return {'array': np.c_[xx.ravel(), yy.ravel()], \n",
    "            'xx': xx, 'yy': yy,\n",
    "            'xmin': xmin, 'xmax': xmax, \n",
    "            'ymin': ymin, 'ymax': ymax}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_and_display(datafile, classifier):\n",
    "    \n",
    "    data = np.loadtxt(datafile)\n",
    "    n, d = data.shape\n",
    "    \n",
    "    # Create training set x and labels y\n",
    "    x = data[:, 0:2]\n",
    "    y = data[:, 2]\n",
    "    \n",
    "    # Train a support vector machine\n",
    "    clf = classifier.fit(x, y)\n",
    "    \n",
    "    # Support vectors\n",
    "    sv = np.zeros(n, dtype=bool)\n",
    "    sv[clf.support_] = True\n",
    "    notsv = np.logical_not(sv)\n",
    "    \n",
    "    # Determine the x1- and x2- limits of the plot\n",
    "    grid = xy_grid(x[:, 0], x[:, 1], ax_pad=1, density=0.05)\n",
    "    grid_xy = grid['array']\n",
    "    plt.xlim(grid['xmin'], grid['xmax'])\n",
    "    plt.ylim(grid['ymin'], grid['ymax'])\n",
    "    \n",
    "    # Plot the data points, enlarging the support vectors  \n",
    "    plt.plot(x[(y==1)*notsv, 0], x[(y==1)*notsv, 1], 'ro')\n",
    "    plt.plot(x[(y==1)*sv,    0], x[(y==1)*sv,    1], 'ro', markersize=10)\n",
    "    plt.plot(x[(y==-1)*notsv, 0], x[(y==-1)*notsv, 1], 'k^')\n",
    "    plt.plot(x[(y==-1)*sv,    0], x[(y==-1)*sv,    1], 'k^', markersize=10)\n",
    "    \n",
    "    Z = clf.decision_function(grid_xy)\n",
    "            \n",
    "    # Show boundary and margin using a color plot\n",
    "    Z = Z.reshape(grid['xx'].shape)\n",
    "    plt.pcolormesh(grid['xx'], grid['yy'], Z, cmap=plt.cm.PRGn, vmin=-2, vmax=2, alpha=.8)\n",
    "    plt.contourf(grid['xx'], grid['yy'], Z, cmap=plt.cm.PRGn, vmin=-2, vmax=2, alpha=.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with the quadratic kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma - boundary width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC(kernel=kernel_type, C=1.0, gamma=1/(sigma**2))\n",
    "kernel = 'rbf'  # distance algo / metric\n",
    "C = 20.0        # misclassification cost; C higher => less tolerance => boundaries less smooth\n",
    "sigma = 20      # margin width; sigma lower => gamma higher => narrower boundary width => less constraint\n",
    "\n",
    "for data in data_files[3:4]:\n",
    "    for sigma in [10e-2, 10e-1, 10e0, 10e1, 10e2]:\n",
    "        print(C, sigma)\n",
    "        learn_and_display(data, SVC(kernel='rbf', C=C, gamma=1.0/sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C - misclassification tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC(kernel=kernel_type, C=1.0, gamma=1/(sigma**2))\n",
    "kernel = 'rbf'\n",
    "C = 20.0\n",
    "sigma = 10\n",
    "\n",
    "for data in data_files[3:4]:\n",
    "    for C in [10e-2, 10e-1, 10e0, 10e1, 10e2]:\n",
    "        print(C, sigma)\n",
    "        learn_and_display(data, SVC(kernel='rbf', C=C, gamma=1.0/sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try `data2.txt` through `data5.txt`. Also try changing the value of `C` (the third parameter) to see how that affects the boundary and margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC(kernel=kernel_type, C=1.0, gamma=1/(sigma**2))\n",
    "kernel = 'rbf'  # distance algo / metric\n",
    "C = 100.0         # slack\n",
    "sigma = 10.0     # soft margin\n",
    "\n",
    "for data in data_files[1:]:\n",
    "    learn_and_display(data, SVC(kernel='rbf', C=C, gamma=1/sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Perceptron\n",
    "\n",
    "- the Perceptron algorithm does not always converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(x, y, degree=None, sigma=5.0, denominator=1):\n",
    "    # Note: denominator factor: 1(from lecture), 2(from wiki)\n",
    "    # 1. vector-vector distance\n",
    "    if x.shape == y.shape:\n",
    "        return np.exp(-np.linalg.norm(x-y)**2 / (denominator*(sigma**2)))\n",
    "    # 2. matrix-vector distances\n",
    "    return np.array([np.exp(-np.linalg.norm(x[i] - y)**2 / (denominator*(sigma**2))) \n",
    "                     for i in range(x.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, y, degree=2, sigma=None):\n",
    "    return (1 + x.dot(y))**degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel(x, y, kernel, degree, sigma, n_iters=1000):\n",
    "    n, d = x.shape\n",
    "    \n",
    "    # nxn kernel similarity matrix\n",
    "    K = np.zeros((n, n))        \n",
    "    for (i,j), _ in np.ndenumerate(K):\n",
    "        K[i,j] = kernel(x[i], x[j], degree, sigma)\n",
    "    \n",
    "    # Random iterations\n",
    "    convergence = 0\n",
    "    alpha, b = np.zeros((n,)), 0\n",
    "    np.random.seed(0)\n",
    "    for itr in range(n_iters):\n",
    "        for idx in np.random.permutation(n):\n",
    "            # Compute no. of misclassifications alpha: -1*1 && 1*-1\n",
    "            if y[idx] * np.sum(alpha * y * K[:,idx] + b) <= 0:\n",
    "                alpha[idx] += 1\n",
    "                b = b + y[idx]\n",
    "                convergence = itr + 1\n",
    "                \n",
    "    print(\"kernel:{}, degree:{}, sigma:{}, {}/{} iterations for convergence\".format(\n",
    "        kernel.__name__, degree, sigma, convergence, n_iters))\n",
    "    \n",
    "    return alpha, b, convergence < n_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and plot kernel perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(datafile=(1, 5), \n",
    "                 kernel={'rbf':rbf, 'poly':poly}, \n",
    "                 degree=(1, 4, 1), \n",
    "                 sigma=widgets.FloatLogSlider(value=1, base=10, min=-3, max=3, step=1, description='sigma'),\n",
    "                 iterations=(100, 1000, 100),\n",
    "                 density=widgets.FloatSlider(value=.1, min=0.05, max=0.25, step=.05, description='plot density'))\n",
    "def interactive_perceptron(datafile, kernel, degree, sigma, iterations, density):\n",
    "    \n",
    "    # Create training set x and labels y\n",
    "    data = np.loadtxt(data_files[datafile])\n",
    "    x = data[:, 0:2]\n",
    "    y = data[:, 2]\n",
    "    \n",
    "    # Train a perceptron\n",
    "    alpha, b, converged = train_kernel(x, y, kernel, degree, sigma, iterations)\n",
    "    \n",
    "    # Plot if converged else stop\n",
    "    if not converged:\n",
    "        print('NOT CONVERGED')\n",
    "        return\n",
    "    \n",
    "    # Determine the x1- and x2- limits of the plot\n",
    "    ax_pad = 1.5\n",
    "    x1min, x1max = min(x[:, 0])-ax_pad, max(x[:, 0])+ax_pad\n",
    "    x2min, x2max = min(x[:, 1])-ax_pad, max(x[:, 1])+ax_pad\n",
    "    plt.xlim(x1min, x1max)\n",
    "    plt.ylim(x2min, x2max)\n",
    "    \n",
    "    # Plot the data points, enlarging those that are support vectors\n",
    "    plt.plot(x[(y==1), 0], x[(y==1), 1], 'ro')\n",
    "    plt.plot(x[(y==-1), 0], x[(y==-1), 1], 'k^')\n",
    "    \n",
    "    # Construct a grid of points and evaluate classifier at each grid points\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1min, x1max, density), np.arange(x2min, x2max, density))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    \n",
    "    # Predict (Z)\n",
    "    Z = np.sign([sum(y * alpha * kernel(x, pt, degree, sigma) + b) for pt in grid])\n",
    "            \n",
    "    # Show boundary and margin using a color plot\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.contourf(xx1, xx2, Z, cmap=plt.cm.PRGn, vmin=-2, vmax=2, alpha=.8)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_and_display_Perceptron(datafile, **kwargs):\n",
    "    kernel, degree, sigma, n_iters = kwargs.values()\n",
    "    \n",
    "    # Create training set x and labels y\n",
    "    data = np.loadtxt(datafile)\n",
    "    x = data[:, 0:2]\n",
    "    y = data[:, 2]\n",
    "    \n",
    "    # Train a perceptron\n",
    "    alpha, b, converged = train_kernel(x, y, kernel, degree, sigma, n_iters)\n",
    "    \n",
    "    # Plot if converged else stop\n",
    "    if not converged:\n",
    "        print('NOT CONVERGED')\n",
    "        return\n",
    "    \n",
    "    # Determine the x1- and x2- limits of the plot\n",
    "    ax_pad = 1.5\n",
    "    x1min, x1max = min(x[:, 0])-ax_pad, max(x[:, 0])+ax_pad\n",
    "    x2min, x2max = min(x[:, 1])-ax_pad, max(x[:, 1])+ax_pad\n",
    "    plt.xlim(x1min, x1max)\n",
    "    plt.ylim(x2min, x2max)\n",
    "    \n",
    "    # Plot the data points, enlarging those that are support vectors\n",
    "    plt.plot(x[(y==1), 0], x[(y==1), 1], 'ro')\n",
    "    plt.plot(x[(y==-1), 0], x[(y==-1), 1], 'k^')\n",
    "    \n",
    "    # Construct a grid of points and evaluate classifier at each grid points\n",
    "    density = 0.1 # 'dpi'\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1min, x1max+density, density), np.arange(x2min, x2max+density, density))\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    \n",
    "    # Predict (Z)\n",
    "    Z = np.sign([sum(y * alpha * kernel(x, pt, degree, sigma) + b) for pt in grid])\n",
    "            \n",
    "    # Show boundary and margin using a color plot\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    # Alternative plot methods\n",
    "#     plt.pcolormesh(xx1, xx2, Z, cmap=plt.cm.PRGn, vmin=-2, vmax=2)\n",
    "    plt.contourf(xx1, xx2, Z, cmap=plt.cm.PRGn, vmin=-2, vmax=2, alpha=.8)\n",
    "#     plt.imshow(Z, aspect='auto', origin='lower', interpolation='none') # TODO\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data in data_files[1:]:\n",
    "    for degree, sigma in zip([1, 2, 3, 4], [0.2, 1, 5, 25]):\n",
    "        for kernel in [rbf, poly]:\n",
    "            print(data)\n",
    "            learn_and_display_Perceptron(data, kernel=kernel, degree=degree, sigma=sigma, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector length and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.array([2,2,2]))\n",
    "np.linalg.norm(np.array([1,1,1]))\n",
    "np.linalg.norm(1)\n",
    "1 - np.array([2,2,2])\n",
    "np.linalg.norm(1 - np.array([2,2,2]))  # broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity/distance from vector to matrix\n",
    "\n",
    "- value 1 means vectors are similar\n",
    "\n",
    "sigma is;\n",
    "- as usually defined in a Gaussian Distribution, the standard deviation\n",
    "- radius around support vectors\n",
    "- [interactive demo](https://cs.stanford.edu/people/karpathy/svmjs/demo/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_(x, y, sigma=3, denominator=2):\n",
    "    \"\"\"\"\"\"\n",
    "    if x.shape == y.shape:\n",
    "        return np.exp(-np.linalg.norm(x-y)**2 / (denominator*(sigma**2)))\n",
    "    return np.array([np.exp(-np.linalg.norm(x[i] - y)**2 / (denominator*(sigma**2))).round(3)\n",
    "                     for i in range(x.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(data_files[5])\n",
    "x, y = data[:, :2], data[:, 2]\n",
    "\n",
    "for sigma in [0.3, 0.5, 1, 2, 4]:\n",
    "    rbf = rbf_(x[:10], x[2], sigma)\n",
    "    print('sigma: {:<3}   vector similarity: {}\\t mean similarity: {:.3f}'.format(sigma, rbf, rbf.mean(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
