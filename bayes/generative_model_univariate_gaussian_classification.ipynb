{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model Classification - Univariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Wine** data set is the running example for our discussion of the *generative approach to classification*. \n",
    "\n",
    "The data can be downloaded from the UCI repository (https://archive.ics.uci.edu/ml/datasets/wine). It contains 178 labeled data points, each corresponding to a bottle of wine:\n",
    "* The features (`x`): a 13-dimensional vector consisting of visual and chemical features for the bottle of wine\n",
    "* The label (`y`): the winery from which the bottle came (1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../.. | grep -i wine.data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Wine data set. There are 178 data points, each with 13 features and a label (1,2,3).\n",
    "We will divide these into a training set of 130 points and a test set of 48 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wine.data.txt' needs to be in the same directory\n",
    "data = np.loadtxt('../../_data/wine.data.txt', delimiter=',')\n",
    "# Names of features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix a particular \"random\" permutation of the data, and use these to effect the training / test split.\n",
    "We get four arrays:\n",
    "* `trainx`: 130x13, the training points\n",
    "* `trainy`: 130x1, labels of the training points\n",
    "* `testx`: 48x13, the test points\n",
    "* `testy`: 48x1, labels of the test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "# Also split data and labels\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "\n",
    "trainx = data[perm[0:130], 1:14]\n",
    "trainy = data[perm[0:130], 0]\n",
    "\n",
    "testx = data[perm[130:178], 1:14]\n",
    "testy = data[perm[130:178], 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many training points there are from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(trainy==1), sum(trainy==2), sum(trainy==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this cell\n",
    "sum(testy==1), sum(testy==2), sum(testy==3)\n",
    "np.bincount(testy.astype('i'))\n",
    "testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature distribution - Gaussian fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(feature=IntSlider(0,0,12), label=IntSlider(1,1,3))\n",
    "def density_plot(feature, label):\n",
    "    plt.hist(trainx[trainy==label, feature], normed=True)\n",
    "    \n",
    "    # Get mean , variance and std. dev.\n",
    "    mu = np.mean(trainx[trainy==label, feature]) # mean\n",
    "    var = np.var(trainx[trainy==label, feature]) # variance\n",
    "    std = np.sqrt(var)                           # standard deviation\n",
    "    \n",
    "    # Plot Gaussian ~N(mu, std)\n",
    "    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis, mu, std), 'r', lw=2)\n",
    "    plt.title(\"Winery \"+str(label) )\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation per feature for given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ = 1\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "sorted([(featurenames[feature], np.std(trainx[trainy==label_, feature])) for feature in range(trainx.shape[1])], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Gaussian generative model to each class\n",
    "\n",
    "fit a Gaussian generative model to the three classes, restricted to just a single feature.\n",
    "\n",
    "- mu = means\n",
    "- var = variances\n",
    "- pi = class weights or PDF or Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generative_model(x, y, feature, labels=[1,2,3]):\n",
    "    n = len(labels)         # number of classes\n",
    "    mu = np.zeros(n)        # list of means\n",
    "    var = np.zeros(n)       # list of variances\n",
    "    pi = np.zeros(n)        # list of class weights (PDF)\n",
    "    for i, label in enumerate(labels):\n",
    "        indices = (y==label)\n",
    "        mu[i] = np.mean(x[indices, feature])\n",
    "        var[i] = np.var(x[indices, feature])\n",
    "        pi[i] = float(sum(indices))/float(len(y))\n",
    "    return labels, mu, var, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 0 # 'alcohol'\n",
    "labels, mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "labels, mu, var, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian distribution for each of the three classes\n",
    "\n",
    " - Seperation of distributions is indicative for the features predictive power.\n",
    " - Good as feature importance/selection tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature=IntSlider(0, 0, 12))\n",
    "def show_densities(feature):\n",
    "    labels, mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    colors = ['r', 'k', 'g']\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        mn = mu[i]\n",
    "        sd = np.sqrt(var[i])\n",
    "        x_axis = np.linspace(mn - 3*sd, mn + 3*sd, 1000)\n",
    "        plt.plot(x_axis, norm.pdf(x_axis, mn, sd), colors[i], label=\"class \" + str(label))\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well can we predict the class (1,2,3) based just on one feature? The code below lets us find this out.\n",
    "\n",
    " - Not prone to overfitting.\n",
    " - Prone to difference in train and test distributions.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature=IntSlider(0, 0, 12))\n",
    "def test_model(feature):\n",
    "    labels, mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    sd = var**.5\n",
    "    colors = ['r', 'k', 'g']\n",
    "    \n",
    "    n = len(labels)\n",
    "    n_train = len(trainy) # Number of test points\n",
    "    n_test = len(testy) # Number of test points\n",
    "    train_score = np.zeros((n_train, n))\n",
    "    test_score = np.zeros((n_test, n))\n",
    "    \n",
    "    # Train score\n",
    "    for i in range(n_train):\n",
    "        for j, label in enumerate(labels):\n",
    "            # max(log(probability of class * probability of feature)) = max(log(Class Weights) + log(PDF))\n",
    "            train_score[i, j] = np.log(pi[j]) + norm.logpdf(trainx[i, feature], mu[j], sd[j])\n",
    "            x_axis = np.linspace(mu[j] - 3*sd[j], mu[j] + 3*sd[j], 1000)\n",
    "            plt.plot(x_axis, norm.pdf(x_axis, mu[j], sd[j]), colors[j], label=\"class \" + str(label))  # \n",
    "        \n",
    "    train_pred = np.asarray(labels)[np.argmax(train_score, axis=1)]\n",
    "    \n",
    "    # Test score  \n",
    "    for i in range(n_test):\n",
    "        for j, label in enumerate(labels):\n",
    "            plt.hist(testx[testy==label, feature], color=colors[j], alpha=.5, normed=True)\n",
    "            \n",
    "            test_score[i, j] = np.log(pi[j]) + norm.logpdf(testx[i, feature], mu[j], sd[j])\n",
    "            x_axis = np.linspace(mu[j] - 3*sd[j], mu[j] + 3*sd[j], 1000)\n",
    "            plt.plot(x_axis, norm.pdf(x_axis, mu[j], sd[j]), colors[j], label=\"class \" + str(label))  # \n",
    "        \n",
    "    test_pred = np.asarray(labels)[np.argmax(test_score, axis=1)]\n",
    "    \n",
    "    # Tally up score\n",
    "    train_score = np.sum(train_pred==trainy)/n_train\n",
    "    test_score = np.sum(test_pred==testy)/n_test\n",
    "    title = 'Feature: {}\\ntrain error: {:.3f}\\ntest error: {:.3f}'.format(featurenames[feature], train_score, test_score)\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Legend without repeat\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = OrderedDict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
