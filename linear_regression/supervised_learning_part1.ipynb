{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning, Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../.. | grep -i fruit_data_with_colors.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_check(X, y):\n",
    "    import numpy as np\n",
    "    assert type(X) == type(np.zeros(2))\n",
    "#     assert X.shape[1] > 0\n",
    "    assert type(y) == type(np.zeros(y.shape))\n",
    "    try:\n",
    "        y.shape[1]\n",
    "        print('{} must be of shape: (n,)'.format(y.shape))  \n",
    "    except:\n",
    "        pass\n",
    "    if len(set(y)) > 9:\n",
    "        classes = 'regression'\n",
    "    else:\n",
    "        classes = set(y)\n",
    "    print('X:\\t {} {}\\ny:\\t {} {}\\nclasses: {}\\n'.format(X.shape, type(X), y.shape, type(y), classes))  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X, y, **kwargs):\n",
    "    import matplotlib.cm as cm\n",
    "    import seaborn as sns\n",
    "    from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "    title = kwargs.get('title', 'default Title')\n",
    "    label = kwargs.get('c', None)\n",
    "    \n",
    "    cmap = kwargs.get('cmap', cm.jet)\n",
    "    colors = ['#FFFF00', '#00AAFF', '#000000', '#FF00AA']\n",
    "    revised = ['#e92929', '#7cf500', '#006990', '#ffa900', '#55c4f6']\n",
    "    col_pal = sns.color_palette(revised).as_hex()\n",
    "    cmap = ListedColormap(col_pal)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.scatter(X, y, c=label, marker= 'o', s=50, cmap=cmap)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntethic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification, make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_R1, y_R1 = make_regression(n_samples=100, n_features=1,\n",
    "                            n_informative=1, bias=150.0,\n",
    "                            noise=30, random_state=0)\n",
    "format_check(X_R1, y_R1)\n",
    "plot_dataset(X_R1, y_R1, title='Sample regression problem with one input variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for more complex regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "format_check(X_F1[:, 2], y_F1)\n",
    "plot_dataset(X_F1[:, 2], y_F1, title='Complex regression problem with one input variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for classification (binary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "\n",
    "format_check(X_C2[:, 0], X_C2[:, 1])\n",
    "plot_dataset(X_C2[:, 0], X_C2[:, 1], c=y_C2, title='Complex regression problem with one input variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for lineary unseparable classification (binary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_D2, y_D2 = make_blobs(n_samples=100, n_features=2, centers=8,\n",
    "                       cluster_std=1.3, random_state=4)\n",
    "y_D2 = y_D2 % 2\n",
    "\n",
    "format_check(X_D2[:,0], X_D2[:,1])\n",
    "plot_dataset(X_D2[:,0], X_D2[:,1], c=y_D2, title='Sample binary classification problem with non-linearly separable classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blobs for classification and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_blob, y_blob = make_blobs(n_samples=500,\n",
    "                              n_features=2, \n",
    "                              centers=2, \n",
    "                              cluster_std=1.0,\n",
    "                              shuffle=True, \n",
    "                              random_state=0)\n",
    "\n",
    "format_check(X_blob, y_blob)\n",
    "plot_dataset(X_blob[:,0], X_blob[:,1], c=y_blob, title='Sample regression problem with one input variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer dataset for classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "format_check(X_cancer, y_cancer)\n",
    "plot_dataset(X_cancer[:,0], X_cancer[:,1], c=y_cancer, title='Breast Cancer dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crime_dataset():\n",
    "    # Communities and Crime dataset for regression\n",
    "    # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "    crime = pd.read_table('../../_data/CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "    \n",
    "    # remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n",
    "    columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n",
    "    crime = crime.iloc[:,columns_to_keep].dropna()\n",
    "\n",
    "    X_crime = crime.iloc[:,range(0,88)]\n",
    "    y_crime = crime['ViolentCrimesPerPop']\n",
    "\n",
    "    return X_crime, y_crime, crime.columns\n",
    "\n",
    "X_crime, y_crime, crime_features = load_crime_dataset()\n",
    "X_crime, y_crime = X_crime.values, y_crime.values\n",
    "\n",
    "format_check(X_crime, y_crime)\n",
    "plot_dataset(X_crime[:,0], X_crime[:,1], c=y_crime, title='Crime dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_table('../../_data/fruit_data_with_colors.txt')\n",
    "\n",
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits].values\n",
    "y_fruits = fruits['fruit_label'].values\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']].values\n",
    "y_fruits_2d = fruits['fruit_label'].values\n",
    "\n",
    "format_check(X_fruits_2d, y_fruits_2d)\n",
    "plot_dataset(X_fruits[:,0], X_fruits[:,1], c=y_fruits, title='Fruits dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fruits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "\n",
    "Accuracy of K-NN classifier on training set: {:.2f}\n",
    "Accuracy of K-NN classifier on test set: {:.2f}\n",
    "'''.format(knn.score(X_train_scaled, y_train), knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_fruit = [[5.5, 2.2, 10, 0.70]]\n",
    "example_fruit_scaled = scaler.transform(example_fruit)\n",
    "\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "          target_names_fruits[knn.predict(example_fruit_scaled)[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_class_knn(X_mat, y_mat, n_neighbors, weights, X_test, y_test):\n",
    "    import matplotlib.patches as mpatches\n",
    "    from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors, weights=weights).fit(X_mat, y_mat)\n",
    "\n",
    "    # Plot the decision boundary by assigning a color in the color map to each mesh point.\n",
    "    mesh_step_size, padding = .01, 1\n",
    "    xx, yy = np.meshgrid(np.arange(X_mat[:, 0].min()-padding, X_mat[:, 0].max()+padding, mesh_step_size),\n",
    "                            np.arange(X_mat[:, 1].min()-padding, X_mat[:, 1].max()+padding, mesh_step_size))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['#FFFFAA', '#AAFFAA', '#AAAAFF','#EFEFEF'])\n",
    "    cmap_bold  = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot training points\n",
    "    plot_symbol_size = 50\n",
    "    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y_mat, cmap=cmap_bold, edgecolor = 'black')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    # Title\n",
    "    title = \"Neighbors = {}\".format(n_neighbors)\n",
    "    if (X_test is not None):\n",
    "        train_score = clf.score(X_mat, y_mat)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Legend\n",
    "    patch0 = mpatches.Patch(color='#FFFF00', label='class 0')\n",
    "    patch1 = mpatches.Patch(color='#000000', label='class 1')\n",
    "    plt.legend(handles=[patch0, patch1])\n",
    "\n",
    "    # Axis labels\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state=0)\n",
    "\n",
    "knnreg = KNeighborsRegressor(n_neighbors=5).fit(X_train, y_train)\n",
    "\n",
    "print(knnreg.predict(X_test))\n",
    "print('R-squared test score: {:.3f}'.format(knnreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 2, figsize=(8,4))\n",
    "for ax, K in zip(subaxes, [1, 3]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    ax.plot(X_predict_input, y_predict_output, '+', markersize=8, label='Predicted', alpha=0.8)\n",
    "    ax.plot(X_train, y_train, 'o', label='True Value', alpha=0.8)\n",
    "    ax.set_xlim([-2.5, 0.75])\n",
    "    ax.set_xlabel('Input feature')\n",
    "    ax.set_ylabel('Target value')\n",
    "    ax.set_title('KNN regression (K={})'.format(K))\n",
    "    ax.legend()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model complexity as a function of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state=0)\n",
    "X_predict_input = np.linspace(-3, 3, 500).reshape(-1,1)\n",
    "\n",
    "\n",
    "fig, subaxes = plt.subplots(2, 3, figsize=(20,10))\n",
    "subaxes = subaxes.ravel()    # !NOTE: 2D axes are sublists [[],[]]\n",
    "for ax, K in zip(subaxes, [1, 3, 7, 11, 15, 55]):\n",
    "    \n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    \n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    train_score = knnreg.score(X_train, y_train)\n",
    "    test_score = knnreg.score(X_test, y_test)\n",
    "    \n",
    "    _ = ax.plot(X_predict_input, y_predict_output)\n",
    "    _ = ax.plot(X_train, y_train, 'o', alpha=0.9, label='Train')\n",
    "    _ = ax.plot(X_test, y_test, '+', alpha=0.9, label='Test')\n",
    "    _ = ax.set_xlabel('Input feature')\n",
    "    _ = ax.set_ylabel('Target value')\n",
    "    _ = ax.set_title('KNN Regression (K={})\\nTrain $R^2 = {:.3f}$,  Test $R^2 = {:.3f}$'\n",
    "                      .format(K, train_score, test_score))\n",
    "    _ = ax.legend()\n",
    "    _ = plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression(model, coef, intercept, train_score, test_score, **kwargs):\n",
    "    non_zero_coef = kwargs.get('non_zero_coef', None)\n",
    "    features = kwargs.get('features', None)\n",
    "    \n",
    "    print('''\n",
    "    model:\\n    {}\n",
    "    linear model coeff (w):      {}\\n\n",
    "    linear model intercept (b):  {:.3f}\n",
    "    R-squared score (training):  {:.3f}\n",
    "    R-squared score (test):      {:.3f}\n",
    "    non-zero coeffs (test):      {}\n",
    "    '''.format(model, coef, intercept, train_score, test_score, non_zero_coef))\n",
    "    \n",
    "    if features is not None:\n",
    "        print('non-zero coeffs sorted by abs weight: ')\n",
    "        coeff_names = sorted(list(zip(features, np.round(coef, 3))), key = lambda e: -abs(e[1]))\n",
    "        for name, coeff in coeff_names:\n",
    "            if coeff != 0:\n",
    "                print('\\t', name, '=', coeff)\n",
    "    else:\n",
    "        coeff_names = None\n",
    "        \n",
    "def print_accuracy(title, train_score, test_score):\n",
    "    print('''\n",
    "    {}\n",
    "    Accuracy on training set: {:.2f}\n",
    "    Accuracy on test set:     {:.2f}'''.format(title, train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state=0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, \n",
    "                 linreg.score(X_train, y_train), linreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression: example plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state=0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, \n",
    "                 linreg.score(X_train, y_train), linreg.score(X_test, y_test), features=crime_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state=0)\n",
    "\n",
    "linreg = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, linreg.score(X_train, y_train), \n",
    "                 linreg.score(X_test, y_test), non_zero_coef=np.sum(linridge.coef_ != 0), features=crime_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state=0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linreg = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, linreg.score(X_train_scaled, y_train), \n",
    "                 linreg.score(X_test_scaled, y_test),  non_zero_coef=np.sum(linridge.coef_ != 0), features=crime_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    \n",
    "    linridge = Ridge(alpha=this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    \n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    \n",
    "    print('''\n",
    "    Alpha = {:.2f}\n",
    "    - non-zero coeffs: {}\n",
    "    - r2 training:     {:.2f}\n",
    "    - r2 test:         {:.2f}'''.format(alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linreg = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, linreg.score(X_train_scaled, y_train), \n",
    "                 linreg.score(X_test_scaled, y_test),  non_zero_coef=np.sum(linridge.coef_ != 0), features=crime_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('''\n",
    "    Alpha = {:.2f}\n",
    "    - non-zero coeffs: {}\n",
    "    - r2 training:     {:.2f}\n",
    "    - r2 test:         {:.2f}'''.format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1, random_state=0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, linreg.score(X_train, y_train), \n",
    "                 linreg.score(X_test, y_test),  non_zero_coef=np.sum(linridge.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial features (degree 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1, random_state=0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, linreg.score(X_train, y_train), \n",
    "                 linreg.score(X_test, y_test),  non_zero_coef=np.sum(linridge.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Polynomial (degree 2)\n",
    "\n",
    "Addition of many polynomial features often leads to overfitting, so we often use polynomial features in combination with regression that has a regularization penalty, like ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1, random_state=0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print_regression(linreg, linreg.coef_, linreg.intercept_, linreg.score(X_train, y_train), \n",
    "                 linreg.score(X_test, y_test),  non_zero_coef=np.sum(linridge.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to binary problem: apples=1, other=0\n",
    "y_fruits_apple = (y_fruits_2d == 1) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_apple, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "\n",
    "h, w = 6, 8\n",
    "print('height {:2}, width {} predicted fruit: {}'\n",
    "     .format(h, w, ['other', 'apple'][clf.predict([[h, w]])[0]]))\n",
    "\n",
    "h, w = 10, 7\n",
    "print('height {:2}, width {} predicted fruit: {}'\n",
    "     .format(h, w, ['other', 'apple'][clf.predict([[h, w]])[0]]))\n",
    "\n",
    "title = 'Logistic regression, binary fruit dataset'.format(1.0)\n",
    "print_accuracy(title, clf.score(X_train, y_train), clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "title = 'Logistic regression for binary classification\\nFruit dataset: Apple vs others'\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)\n",
    "subaxes.set_xlabel('height')\n",
    "subaxes.set_ylabel('width');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression on simple synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
    "print_accuracy(title, clf.score(X_train, y_train), clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                         None, None, title, subaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression regularization: C hyperparameter\n",
    "- C penalty regulator on coeff weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_apple, random_state=0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for C, subplot in zip([0.1, 1, 100], subaxes):\n",
    "    clf = LogisticRegression(C=C).fit(X_train, y_train)\n",
    "    title ='Logistic regression (apple vs rest), C = {:.3f}'.format(C)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test, y_test, title, subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "title = 'Breast cancer dataset'\n",
    "print_accuracy(title, clf.score(X_train, y_train), clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from adspy_shared_utilities import plot_class_regions_for_classifier_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "C = 1.0\n",
    "clf = SVC(kernel = 'linear', C=C).fit(X_train, y_train)\n",
    "title = 'Linear SVC, C = {:.3f}'.format(C)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machine: C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "fig, subaxes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for C, subplot in zip([0.00001, 0.1, 100], subaxes):\n",
    "    clf = LinearSVC(C=C).fit(X_train, y_train)\n",
    "    title = 'Linear SVC, C = {:.5f}'.format(C)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LinearSVC().fit(X_train, y_train)\n",
    "title = 'Breast cancer dataset'\n",
    "print_accuracy(title, clf.score(X_train, y_train), clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC with M classes generates M one vs rest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n",
    "\n",
    "clf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)\n",
    "print('Coefficients:\\n', clf.coef_)\n",
    "print('Intercepts:\\n', clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class results on the fruit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n",
    "\n",
    "plt.scatter(X_fruits_2d[:,0], X_fruits_2d[:,1],\n",
    "           c=y_fruits_2d, cmap=cmap_fruits, edgecolor = 'black', alpha=.7)\n",
    "\n",
    "x_0_range = np.linspace(-10, 15)\n",
    "\n",
    "for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n",
    "    # Since class prediction with a linear model uses the formula y = w_0 x_0 + w_1 x_1 + b, \n",
    "    # and the decision boundary is defined as being all points with y = 0, to plot x_1 as a \n",
    "    # function of x_0 we just solve w_0 x_0 + w_1 x_1 + b = 0 for x_1:\n",
    "    plt.plot(x_0_range, -(x_0_range * w[0] + b) / w[1], c=color, alpha=.8)\n",
    "    \n",
    "plt.legend(target_names_fruits)\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('width')\n",
    "plt.xlim(-2, 12)\n",
    "plt.ylim(-2, 15)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "models = [SVC(), SVC(kernel = 'poly', degree=2), SVC(kernel = 'poly', degree=4)]\n",
    "titles = ['SVC - RBF kernel' , 'SVC - Polynomial kernel degree 2', 'SVC - Polynomial kernel degree 4']\n",
    "\n",
    "for model, title, ax in zip(models, titles, subaxes):\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, ax)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with RBF kernel: gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "fig, subaxes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for this_gamma, ax in zip([0.01, 1.0, 10.0], subaxes):\n",
    "    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n",
    "    title = 'SVC: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, ax)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with RBF kernel: using both C and gamma parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "fig, subaxes = plt.subplots(3, 4, figsize=(20, 16), dpi=50)\n",
    "\n",
    "for gamma, axis in zip([0.01, 1, 5], subaxes):\n",
    "    for C, ax in zip([0.1, 1, 15, 250], axis):\n",
    "        title = 'gamma = {:.2f}, C = {:.2f}'.format(gamma, C)\n",
    "        clf = SVC(kernel = 'rbf', gamma=gamma, C=C).fit(X_train, y_train)\n",
    "        plot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test, y_test, title,ax)\n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of SVMs to a real dataset: unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train, y_train)\n",
    "\n",
    "print_accuracy('Breast cancer dataset (unnormalized features)', clf.score(X_train, y_train), clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train_scaled, y_train)\n",
    "\n",
    "title = 'Breast cancer dataset (normalized with MinMax scaling)'\n",
    "print_accuracy(title, clf.score(X_train_scaled, y_train), clf.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example based on k-NN classifier with fruit dataset (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "X = X_fruits_2d\n",
    "y = y_fruits_2d\n",
    "cv_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "print('Cross-validation scores (3-fold):', cv_scores)\n",
    "print('Mean cross-validation score (3-fold): {:.3f}'\n",
    "     .format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on performing cross-validation for more advanced scenarios.\n",
    "\n",
    "In some cases (e.g. when feature values have very different ranges), we've seen the need to scale or normalize the training and test sets before use with a classifier. The proper way to do cross-validation when you need to scale the data is *not* to scale the entire dataset with a single transform, since this will indirectly leak information into the training data about the whole dataset, including the test data (see the lecture on data leakage later in the course).  Instead, scaling/normalizing must be computed and applied for each cross-validation fold separately.  To do this, the easiest way in scikit-learn is to use *pipelines*.  While these are beyond the scope of this course, further information is available in the scikit-learn documentation here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "or the Pipeline section in the recommended textbook: Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido (O'Reilly Media)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation curve example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = np.logspace(-3, 3, 4)\n",
    "train_scores, test_scores = validation_curve(SVC(), X, y,\n",
    "                                            param_name='gamma',\n",
    "                                            param_range=param_range, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "#  See:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with SVM')\n",
    "plt.xlabel('$\\gamma$ (gamma)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
