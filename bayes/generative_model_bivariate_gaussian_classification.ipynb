{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model Classification - Bivariate Gaussian\n",
    "\n",
    "Our first generative model for Winery classification used just one feature. Now we use two features, modeling each class by a **bivariate Gaussian**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of two features from one of the wineries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to plot the distribution of two features from a particular winery. We will use several helper functions for this. It is worth understanding each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal \n",
    "\n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set plot defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import rcParams\n",
    "\n",
    "mpl.rc('figure', figsize=[8., 8.])\n",
    "mpl.rc('axes.spines', left=False, top=False, right=False, bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "data = np.loadtxt('../../_data/wine.data.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']\n",
    "feature_names = {f:i for i, f in enumerate(featurenames)}\n",
    "feature_lookup = {i:f for i, f in enumerate(featurenames)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target and labels\n",
    "\n",
    " - the target is in column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:, 0].astype('int8')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelnames = ['Winery 1', 'Winery 2', 'Winery 3']\n",
    "label_names = {f:i for i, f in enumerate(labelnames)}\n",
    "label_lookup = {i:f for i, f in enumerate(labelnames)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 1:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "\n",
    "# training set (trainx, trainy) of size 130/178\n",
    "trainx = X[perm[0:130], :]\n",
    "trainy = y[perm[0:130]]\n",
    "\n",
    "# test set (testx, testy) of size 48/178\n",
    "testx = X[perm[130:178], :]\n",
    "testy = y[perm[130:178]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Gaussian to a dataset feature\n",
    "\n",
    " - function returns the mean and covariance matrix of the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian(x, features):\n",
    "    mu = np.mean(x[:, features], axis=0)\n",
    "    sigma = np.cov(x[:, features], rowvar=0, bias=1)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's look at the Gaussian we get for winery 1, using features 0 ('alcohol') and 6 ('flavanoids')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, f2, label = 0, 6, 1\n",
    "mu, sigma = fit_gaussian(trainx[trainy==label, :], [f1, f2])\n",
    "print(\"Mean: \" + str(mu), \"\\n\")\n",
    "print(\"Covariance matrix:\\n\" + str(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that will fit a Gaussian generative model to the three classes, restricted to a given list of features. The function returns:\n",
    "* `mu`: the means of the Gaussians, one per row\n",
    "* `covar`: covariance matrices of each of the Gaussians\n",
    "* `pi`: list of three class weights summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes y takes on values 1,2,3\n",
    "def fit_generative_model(x, y, features):\n",
    "    n = len(label_lookup)        # number of classes\n",
    "    d = len(features)            # number of features\n",
    "    mu = np.zeros((n, d))        # list of means\n",
    "    sigma = np.zeros((n, d, d))  # list of covariance matrices\n",
    "    pi = np.zeros(n)             # list of class weights (PDF)\n",
    "    \n",
    "    for label in label_lookup:\n",
    "        indices = (y==label)\n",
    "        mu[label, :], sigma[label, :,:] = fit_gaussian(x[indices, :], features)\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, sigma, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for plotting\n",
    "\n",
    "Next, we will construct a routine for displaying points sampled from a two-dimensional Gaussian, as well as a few contour lines. Part of doing this involves deciding what range to use for each axis. We begin with a little helper function that takes as input an array of numbers (values along a single feature) and returns the range in which these numbers lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range within which an array of numbers lie, with a little buffer\n",
    "def find_range(x, margin=0.3):\n",
    "    lower = min(x)\n",
    "    upper = max(x)\n",
    "    band = upper - lower\n",
    "    lower = lower - margin * band\n",
    "    upper = upper + margin * band\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a routine that plots a few contour lines of a given two-dimensional Gaussian.\n",
    "It takes as input:\n",
    "* `mu`, `cov`: the parameters of the Gaussian\n",
    "* `x1g`, `x2g`: the grid (along the two axes) at which the density is to be computed\n",
    "* `col`: the color of the contour lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(mu, sigma, x1g, x2g, col):\n",
    "    rv = multivariate_normal(mean=mu, cov=sigma)\n",
    "    z = np.zeros((len(x1g), len(x2g)))\n",
    "    \n",
    "    for i in range(len(x1g)):\n",
    "        for j in range(len(x2g)):\n",
    "            z[j,i] = rv.logpdf([x1g[i], x2g[j]])\n",
    "            \n",
    "    # Normalise contours\n",
    "    sign, logdet = np.linalg.slogdet(sigma)\n",
    "    normalizer = -0.5 * (2*np.log(6.28) + sign*logdet)\n",
    "    \n",
    "    for offset in range(1, 4):\n",
    "        plt.contour(x1g, x2g, z, levels=[normalizer - offset], colors=col, linewidths=2.0, linestyles='solid')\n",
    "#         plt.contour(x1g, x2g, z, colors=col, linestyles='solid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate contour and distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature_1=feature_names, feature_2=feature_names, label=label_names)\n",
    "def two_features_plot(feature_1, feature_2, label):\n",
    "    \n",
    "    f1, f2 = feature_1, feature_2\n",
    "    \n",
    "    if f1 == f2:\n",
    "        print(\"Please choose different features for axis\")\n",
    "        return\n",
    "\n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[trainy==label, f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[trainy==label, f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "#     plt.gca().set_aspect('equal', 'datalim')\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    plt.plot(trainx[trainy==label, f1], trainx[trainy==label, f2], 'ro')\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Now plot a few contour lines of the density\n",
    "    mu, cov = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
    "    plot_contours(mu, cov, x1g, x2g, 'k')\n",
    "    \n",
    "    # Finally, display\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    title = '{}'.format(label_lookup[label])\n",
    "    plt.title(title, fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class bivariate contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature_1=feature_names, feature_2=feature_names)\n",
    "def three_class_plot(feature_1, feature_2):\n",
    "    \n",
    "    f1, f2 = feature_1, feature_2\n",
    "    \n",
    "    if f1 == f2:\n",
    "        print(\"Please choose different features for axis\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "#     plt.gca().set_aspect('equal', 'datalim')\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in label_lookup.keys():\n",
    "        plt.plot(trainx[trainy==label, f1], trainx[trainy==label, f2], marker='o', ls='None', c=colors[label])\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Show the Gaussian fit to each class, using features f1,f2\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1, f2])\n",
    "    \n",
    "    for label in label_lookup.keys():\n",
    "        gmean = mu[label, :]\n",
    "        gcov = covar[label, :,:]\n",
    "        plot_contours(gmean, gcov, x1g, x2g, colors[label])\n",
    "\n",
    "    # Finally, display\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Wine data', fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for the test points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well we can predict the class (1,2,3) based just on these two features?\n",
    "\n",
    "We start with a testing procedure that is analogous to what we developed in the 1-d case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test the performance of a predictor based on a subset of features\n",
    "@interact(feature_1=feature_names, feature_2=feature_names)\n",
    "def test_model(feature_1, feature_2, print_out=True):\n",
    "    \n",
    "    f1, f2 = feature_1, feature_2\n",
    "    \n",
    "    if f1 == f2: # need f1 != f2\n",
    "        print(\"Please choose different features for axis\")\n",
    "        return  \n",
    "    features = [f1,f2]\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    \n",
    "    score = np.zeros((len(testy), len(label_lookup)))\n",
    "    \n",
    "    for i in range(len(testy)):\n",
    "        for label in label_lookup:\n",
    "            # max(log(probability of class * probability of feature)) = max(log(Class Weights) + log(PDF))\n",
    "            # highest probability => label\n",
    "            score[i, label] = np.log(pi[label]) + multivariate_normal.logpdf(\n",
    "                testx[i, features], mean=mu[label, :], cov=covar[label, :,:])\n",
    "            \n",
    "    predictions = np.argmax(score, axis=1)\n",
    "    \n",
    "    # Sum up errors\n",
    "    errors = np.sum(predictions!=testy)\n",
    "    if print_out:\n",
    "        print('Features: {}, {}, test errors: {:.0f}/{}'.format(feature_lookup[f1], feature_lookup[f2], errors, score.shape[0]))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Feature pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different pairs of features yield different test errors.\n",
    "* What is the smallest achievable test error?\n",
    "* Which pair of features achieves this minimum test error?\n",
    "\n",
    "*Make a note of your answers to these questions, as you will need to enter them as part of this week's assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = 100\n",
    "ftrs = None\n",
    "for f1 in range(len(feature_lookup)):\n",
    "    for f2 in range(len(feature_lookup)):\n",
    "        if f1==f2: continue\n",
    "        errors = test_model(f1, f2, print_out=False)\n",
    "        if errors < r_min:\n",
    "            r_min = min(r_min, errors)\n",
    "            ftrs = feature_lookup[f1], feature_lookup[f2]\n",
    "print('Features with lowest test error: {} & {}, test error: {}'.format(ftrs[0], ftrs[1], r_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive decision boundary on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature_1=feature_names, feature_2=feature_names)\n",
    "def show_decision_boundary(feature_1, feature_2, train_data_on=True, test_data_on=True):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    cm_ = ListedColormap(['r', 'k', 'g'])\n",
    "    \n",
    "    f1, f2 = feature_1, feature_2\n",
    "    \n",
    "    if f1 == f2: return\n",
    "    \n",
    "    # Fit Gaussian to each class\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1, f2])\n",
    "    \n",
    "    # Set up dimensions of plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:, f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:, f2])\n",
    "    plt.xlim([x1_lower, x1_upper])\n",
    "    plt.ylim([x2_lower, x2_upper])\n",
    "\n",
    "    # Plot points in training set\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in label_lookup:\n",
    "        if train_data_on:\n",
    "            plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label])\n",
    "        if test_data_on:\n",
    "            plt.plot(testx[testy==label, f1], testx[testy==label, f2], marker='*', ls='None', c=colors[label])\n",
    "        \n",
    "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Declare Random Variables corresponding to each class density\n",
    "    random_vars = {}\n",
    "    for label in label_lookup:\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label, :], cov=covar[label, :, :])\n",
    "\n",
    "    # Classify every point in the grid; these are stored in an array Z[]\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    \n",
    "    for i in range(len(x1g)):\n",
    "        for j in range(len(x2g)):\n",
    "            scores = []\n",
    "            for label in label_lookup:\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i], x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) #+1\n",
    "\n",
    "    # Test errors\n",
    "    errors = test_model(f1, f2, print_out=False)\n",
    "    \n",
    "    # Plot the contour lines\n",
    "    # contour: only contours, contourf: filled \n",
    "    plt.contourf(x1g, x2g, Z.T, 3, cmap=cm_, alpha=.3)\n",
    "    \n",
    "    # Finally, show the image\n",
    "    plt.xlabel(feature_lookup[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(feature_lookup[f2], fontsize=14, color='red')\n",
    "    plt.title('Decision boundary\\ntest errors: {}/{}'.format(errors, len(testy)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
