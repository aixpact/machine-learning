{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we get an understanding of **principal component analysis (PCA)** using the familiar MNIST data set of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few routines check if the MNIST data is already in the current directory; if not, it is downloaded directly from Yann Le Cun's web site. It is then loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import gzip, sys, os\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    from urllib import urlretrieve\n",
    "else:\n",
    "    from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "    print(\"Downloading %s\" % filename)\n",
    "    urlretrieve(source + filename, filename)\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    # Read the inputs in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1,784)\n",
    "    return data / np.float32(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_mnist_images('../../_data/train-images-idx3-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape\n",
    "n_vector = train_data.shape[1]\n",
    "np.min(train_data), np.max(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis chooses projection directions based on the **covariance matrix** of the data.   \n",
    "This matrix allows us to contrast the effect of picking coordinate directions (i.e. pixels) versus eigenvector directions. In particular:\n",
    "* *The ith **diagonal entry** of the covariance is the variance in the ith coordinate (the ith pixel).*\n",
    "* *The ith **eigenvalue** of the covariance matrix is the variance in the direction of the ith eigenvector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix\n",
    "sigma = np.cov(train_data, rowvar=0, bias=1)\n",
    "\n",
    "# Compute coordinate-wise variances, in increasing order\n",
    "coordinate_variances = np.sort(sigma.diagonal())\n",
    "\n",
    "# Compute variances in eigenvector directions, in increasing order\n",
    "eigenvector_variances = np.sort(np.linalg.eigvalsh(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the (substantial) benefit of eigenvector projections over coordinate projections, we create a plot that shows the variance lost due to each of these.\n",
    "\n",
    "For each `k` (projection dimension), we compute:\n",
    "* How much of the overall variance is lost when we project to the best `k` coordinate directions?\n",
    "* How much of the overall variance is lost when we project to the top `k` eigenvectors (as in PCA)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fraction of overall variance lost when projecting to k coordinate directions\n",
    "# cumsum() returns array of cumulative sums, last entry [783] is total sum\n",
    "total_coordinate_variance = np.cumsum(coordinate_variances)\n",
    "total_coordinate_variance = total_coordinate_variance/total_coordinate_variance[n_vector-1]\n",
    "\n",
    "# Compute fraction of overall variance lost when projecting to k eigenvector directions\n",
    "total_eigenvector_variance = np.cumsum(eigenvector_variances)\n",
    "total_eigenvector_variance = total_eigenvector_variance/total_eigenvector_variance[n_vector-1]\n",
    "\n",
    "# Plot these results\n",
    "plt.plot(np.arange(1, n_vector), total_coordinate_variance[784:0:-1], 'b-', lw=2)\n",
    "plt.plot(np.arange(1, n_vector), total_eigenvector_variance[784:0:-1], 'r-', lw=2)\n",
    "\n",
    "plt.xlabel('projection dimension', fontsize=14)\n",
    "plt.ylabel('fraction of residual variance', fontsize=14)\n",
    "plt.xlim(0, n_vector)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.legend(['coordinate directions', 'PCA directions'], fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection and reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a more *visual* feel for what information is lost during dimensionality reduction.\n",
    "\n",
    "Suppose we find the PCA projection to `k` dimensions. What is the result of:\n",
    "* Starting with a handwritten digit in the original (784-dimensional) space\n",
    "* *Projecting* it down to `k` dimensions\n",
    "* *Reconstructing* an image in 784-dimensional space from this `k`-dimensional projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors of the covariance matrix\n",
    "\n",
    " - `numpy.linalg.eigh` returns Eigenvalues and Eigenvectors in order of increasing Eigenvalue.  \n",
    " - The Eigenvectors are normalized to unit length and returned as columns of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eigh(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection followed by reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`U`** = (784 x `k`) matrix, whose columns are the top `k` eigenvectors;\n",
    " - **`transpose(U)`** performs the PCA projection onto the top `k` directions\n",
    " - **`U`** reconstructs a point in the original space from its `k` dimensional projection\n",
    "\n",
    "The dotproduct `U @ transpose(U)` is a (784 x 784) matrix that does a **projection-followed-by-reconstruction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the project-and-reconstruct operations as a single matrix\n",
    "def projection_and_reconstruction(k):\n",
    "    U = eigenvectors[:, (n_vector-k):n_vector]\n",
    "    return U @ U.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next routine displays an handwritten digit image given as a 784-dimensional vector. It begins by clipping each entry to lie in the range [0,255]; the images returned after PCA reconstruction might not satisfy this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(x, ax, k):\n",
    "    \n",
    "    # Clip all entries of x to range [0, 255]\n",
    "    for i in range(n_vector):\n",
    "        x[i] = max(0.0, x[i])\n",
    "        x[i] = min(255.0, x[i])\n",
    "        \n",
    "    # Now display\n",
    "    ax.axis('off')\n",
    "    ax.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    ax.set_title('Projection dim.: {}'.format(k))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_effect_of_PCA(x, k_list):\n",
    "    fig, axes = plt.subplots(1, len(k_list), figsize=(18, 6))\n",
    "    \n",
    "    for k, ax in zip(k_list, axes):\n",
    "        P = projection_and_reconstruction(k)\n",
    "        show_digit(P @ x, ax, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.permutation(range(n_vector))[:5]\n",
    "\n",
    "for i in sample_indices:\n",
    "    show_effect_of_PCA(train_data[i,], [n_vector, 64, 32, 16, 8, 4, 2])\n",
    "    plt.subplots_adjust(wspace=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olivetti Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_olivetti_faces()\n",
    "faces = dataset.data\n",
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise PCA compressed faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_and_reconstruction(data, image_shape, k):\n",
    "    \"\"\"Return the project-and-reconstruct operations as a single matrix\"\"\"\n",
    "    n_vector = image_shape[0] * image_shape[1]\n",
    "    sigma = np.cov(data, rowvar=0, bias=1)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(sigma)\n",
    "    U = eigenvectors[:, (n_vector-k):n_vector]\n",
    "    return U @ U.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_face(x, image_shape, ax, k):\n",
    "    \"\"\"\"\"\"\n",
    "    # Now display\n",
    "    ax.axis('off')\n",
    "    ax.imshow(x.reshape(image_shape), cmap=plt.cm.gray)\n",
    "    ax.set_title('Projection dim.: {}'.format(k))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_effect_of_PCA(data, x, image_shape, k_list):\n",
    "    \"\"\"\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(k_list), figsize=(18, 6))\n",
    "    \n",
    "    for k, ax in zip(k_list, axes):\n",
    "        P = projection_and_reconstruction(data, image_shape, k)\n",
    "        show_face(P @ x, image_shape, ax, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.permutation(range(faces.shape[0]))[:10]\n",
    "\n",
    "for i in sample_indices:\n",
    "    show_effect_of_PCA(faces, faces[i], image_shape, [512, 256, 128, 64, 32, 16, 8, 4])\n",
    "    plt.subplots_adjust(wspace=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces[0].shape\n",
    "np.eye(64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Compute the qr factorization of a matrix.\n",
    "#     Factor the matrix a as qr, where q is orthonormal and r is upper-triangular.\n",
    "E, _ = np.linalg.qr(B)\n",
    "E = E.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_matrix(B) : \n",
    "    \n",
    "    # The parameter B is a basis is a 2Ã—2 matrix that is passed to the function.\n",
    "    \n",
    "    # Use the gsBasis function on bearBasis to get the mirror's orthonormal basis.\n",
    "\n",
    "#     E = gsBasis(bearBasis)\n",
    "    \n",
    "    # Transformation matrix that perform's the mirror's reflection in the mirror's basis.\n",
    "    TE = np.array([[1, 0],\n",
    "                   [0, -1]])\n",
    "    # Combine the matrices E and TE to produce your transformation matrix.\n",
    "    T = E @ TE @ E.T\n",
    "    return T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = transformation_matrix(np.eye(10))\n",
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T @ faces.reshape(-1,64)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, _ = np.linalg.qr(faces[0].reshape(-1,64))\n",
    "E = E.reshape(-1, 64)\n",
    "E.shape\n",
    "(E @ np.eye(64) @ E.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "face = faces[0].reshape(-1, dim)\n",
    "t_face = face @ np.eye(dim) @ face.T\n",
    "plt.imshow(t_face.reshape(-1, 64), cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_face_((E @ np.eye(64) @ E.T), (-1,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
