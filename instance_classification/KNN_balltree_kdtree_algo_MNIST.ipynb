{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbor for handwritten digit recognition\n",
    "\n",
    "In this notebook we will build a classifier that takes an image of a handwritten digit and outputs a label 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "# import os\n",
    "# os.getcwd()\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset\n",
    "\n",
    "`MNIST` is a classic dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The original training set contains 60,000 examples and the test set contains 10,000 examples. In this notebook we will be working with a subset of this data: a training set of 7,500 examples and a test set of 1,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !find ../../_data | grep -i mnist/\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the training set\n",
    "train_data = np.load('../../_data/MNIST/train_data.npy')\n",
    "train_labels = np.load('../../_data/MNIST/train_labels.npy')\n",
    "\n",
    "## Load the testing set\n",
    "test_data = np.load('../../_data/MNIST/test_data.npy')\n",
    "test_labels = np.load('../../_data/MNIST/test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data))\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data))\n",
    "print(\"Number of testing labels: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_digits, train_counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Training set distribution:\")\n",
    "print(dict(zip(train_digits, train_counts)))\n",
    "\n",
    "test_digits, test_counts = np.unique(test_labels, return_counts=True)\n",
    "print(\"Test set distribution:\")\n",
    "print(dict(zip(test_digits, test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Each data point is stored as 784-dimensional vector. To visualize a data point, we first reshape it to a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that displays a digit given its vector representation\n",
    "def show_digit(x, **kwargs):\n",
    "    ax = kwargs.get('ax', plt.gca())\n",
    "    ax.axis('off')\n",
    "    ax.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    return\n",
    "\n",
    "## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\n",
    "def vis_image(index, dataset=\"train\"):\n",
    "    if(dataset==\"train\"): \n",
    "        show_digit(train_data[index,])\n",
    "        label = train_labels[index]\n",
    "    else:\n",
    "        show_digit(test_data[index,])\n",
    "        label = test_labels[index]\n",
    "    print(\"Label \" + str(label))\n",
    "    return\n",
    "\n",
    "## View the first data point in the training set\n",
    "vis_image(0, \"train\")\n",
    "plt.show()\n",
    "\n",
    "## Now view the first data point in the test set\n",
    "vis_image(0, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN distance metrics\n",
    "\n",
    "To compute nearest neighbors in our data set, we need to first be able to compute distances between data points. \n",
    "A natural distance function is _Euclidean distance aka the L2-norm_: for two vectors $x, y \\in \\mathbb{R}^d$, their Euclidean distance is defined as   \n",
    "$$L2-norm = \\|x - y\\|^2 = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.$$\n",
    "\n",
    "Often we omit the square root and simply compute _squared Euclidean distance_:\n",
    "$$\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.$$\n",
    "\n",
    "Another distance metric is the _Manhattan distance or taxicab distance aka the L1-norm_:   \n",
    "$$L1-norm = \\|x - y\\| = \\sum_{i=1}^d |x_i - y_i|.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(x, y):\n",
    "    \"\"\"L1 (Manhattan) distance.\n",
    "    :param x, y: vectors x, y\n",
    "    :returns: distance\"\"\"\n",
    "    return np.sum(np.abs(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(x, y):\n",
    "    \"\"\"L2 (Euclidean) distance.\n",
    "    :param x, y: vectors x, y\n",
    "    :returns: distance\"\"\"\n",
    "    return np.sum(np.square(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_norm(x, y, norm):\n",
    "    \"\"\"L1 (Manhattan) distance.\n",
    "    :param x, y: vectors x, y\n",
    "    :returns: distance\"\"\"\n",
    "    return norm(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute distance between a seven and a one in our training set.\n",
    "print(\"Distance from 7 to 1: \", L_norm(train_data[4,], train_data[5,], L2))\n",
    "print(\"Distance from 7 to 1: \", L_norm(train_data[4,], train_data[5,], L1))\n",
    "\n",
    "## Compute distance between a seven and a two in our training set.\n",
    "print(\"Distance from 7 to 2: \", L_norm(train_data[4,], train_data[1,], L2))\n",
    "print(\"Distance from 7 to 2: \", L_norm(train_data[4,], train_data[1,], L1))\n",
    "\n",
    "## Compute distance between two seven's in our training set.\n",
    "print(\"Distance from 7 to 7: \", L_norm(train_data[4,], train_data[7,], L2))\n",
    "print(\"Distance from 7 to 7: \", L_norm(train_data[4,], train_data[7,], L1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing nearest neighbors\n",
    "\n",
    "Now that we have a distance function defined, we can now turn to nearest neighbor classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a vector x and returns the index of its nearest neighbor in train_data\n",
    "def find_NN(x, norm):\n",
    "    # Compute distances from x to every row in train_data\n",
    "    distances = [L_norm(x, train_data[i,], norm) for i in range(len(train_labels))]\n",
    "    # Get the index of the smallest distance (in train)\n",
    "    return np.argmin(distances)\n",
    "\n",
    "## Takes a vector x and returns the class of its nearest neighbor in train_data\n",
    "def NN_classifier(x, norm):\n",
    "    # Get the index of the the nearest neighbor\n",
    "    index = find_NN(x, norm)\n",
    "    # Return its class\n",
    "    return train_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_test(index, norm):\n",
    "    \"\"\"\n",
    "    :param: index of test_data\n",
    "    :returns: None\n",
    "    :plot: images of test and predicted labels\"\"\"\n",
    "    true_title = '''\n",
    "    Test label: {}'''.format(\n",
    "        test_labels[index])\n",
    "    \n",
    "    pred_title = '''\n",
    "    NN Train label: {}\n",
    "    Correct class: {}'''.format(\n",
    "        NN_classifier(test_data[index, ], norm), \n",
    "        test_labels[index]==NN_classifier(test_data[index, ], norm))\n",
    "    \n",
    "#     print(title)\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].set_title(true_title)\n",
    "    axes[1].set_title(pred_title)\n",
    "    show_digit(test_data[index,], ax=axes[0])\n",
    "    show_digit(train_data[find_NN(test_data[index, ], norm), ], ax=axes[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_test(39, L1)\n",
    "NN_test(39, L2)\n",
    "NN_test(100, L1)\n",
    "NN_test(100, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.permutation(range(test_data.shape[0]))[:10]:\n",
    "    NN_test(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify full test set\n",
    "\n",
    "Note that to classify __each test point__, our code takes a __full pass over each of the 7500 training examples__. Thus we should not expect testing to be very fast. The following code takes about 50-60 seconds on MacBookPro 2016 (2,9 GHz Intel Core i5, 8 GB 2133 MHz LPDDR3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_before = time.time()\n",
    "test_predictions = [NN_classifier(test_data[i, ]) for i in range(len(test_labels))]\n",
    "t_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the error\n",
    "err_positions = np.not_equal(test_predictions, test_labels)\n",
    "error = float(np.sum(err_positions))/len(test_labels)\n",
    "\n",
    "print(\"Error of nearest neighbor classifier: \", error)\n",
    "print(\"Classification time (seconds): \", t_after - t_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster NN algorithms\n",
    "\n",
    "Performing nearest neighbor classification in the way we have presented requires a full pass through the training set in order to classify a single point. If there are $N$ training points in $\\mathbb{R}^d$, this takes $O(N d)$ time.\n",
    "\n",
    "Fortunately, there are faster methods to perform nearest neighbor look up if we are willing to spend some time preprocessing the training set. __`scikit-learn`__ has __fast implementations of two useful nearest neighbor data structures: the _ball tree_ and the _k-d tree___. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ball tree\n",
    "\n",
    "A tree implementation of the training set. Traversing through a tree is computationaly more efficient.\n",
    "First the training set must be converted to a tree, this initial cost is fractional and will be paid back in every search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build nearest neighbor structure on training data\n",
    "t_before = time.time()\n",
    "ball_tree = BallTree(train_data)\n",
    "t_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute training time\n",
    "t_training = t_after - t_before\n",
    "print(\"Time to build data structure (seconds): \", t_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get nearest neighbor predictions on testing data\n",
    "t_before = time.time()\n",
    "test_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))\n",
    "ball_tree_predictions = train_labels[test_neighbors]\n",
    "t_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute testing time\n",
    "t_testing = t_after - t_before\n",
    "print(\"Time to classify test set (seconds): \", t_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify that the predictions are the same\n",
    "print(\"Ball tree produces same predictions as above? \", np.array_equal(test_predictions, ball_tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build nearest neighbor structure on training data\n",
    "t_before = time.time()\n",
    "kd_tree = KDTree(train_data)\n",
    "t_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute training time\n",
    "t_training = t_after - t_before\n",
    "print(\"Time to build data structure (seconds): \", t_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get nearest neighbor predictions on testing data\n",
    "t_before = time.time()\n",
    "test_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\n",
    "kd_tree_predictions = train_labels[test_neighbors]\n",
    "t_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute testing time\n",
    "t_testing = t_after - t_before\n",
    "print(\"Time to classify test set (seconds): \", t_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify that the predictions are the same\n",
    "print(\"KD tree produces same predictions as above? \", np.array_equal(test_predictions, kd_tree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
